{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/main/Seminars/lab_03_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5jNnHqhFC-b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab as plt\n",
        "from sklearn.datasets import make_classification\n",
        "%pylab inline\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "pylab.rcParams['figure.figsize'] = (12.0, 5.0)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbyn2S0RFC-d"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEI1v2joFC-d"
      },
      "source": [
        "## Linear SVM reminder\n",
        "\n",
        "Let's look at binary classification problem. Training samples are given by $\\{(x_n, y_n)\\}_{n=1}^N$, where $N$ — number of objects, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — feature vector of object, $y_n \\in \\{+1, -1\\}$ — class of object.\n",
        "\n",
        "SVM trains model for separating hyperplane:\n",
        "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
        "Parameters of model — vector of weights $\\boldsymbol w \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$.\n",
        "\n",
        "Training is done by solving optimisation problem:\n",
        "$$\n",
        "\\begin{gather}\n",
        "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
        "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Constraint $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ assures that objects are correctly classified by separating hyperplane. Since in practice the sample could not be linearly separable the slack variables $\\xi_n$ are introduced , which weakens condition of right classification. $\\| \\boldsymbol w \\|^2$ penalise small width of margin,  $\\sum_n \\xi_n$ penalise weakens of constraints. \n",
        "\n",
        "The solution of optimisation problem is given by $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, some of the constraints become active, i.e. become a exact equality:\n",
        "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
        "Objects, corresponding to active constraints called $\\textbf{support vectors}$.\n",
        "\n",
        "\n",
        "Hyperparameter $C$ is responsible of balancing the width of margin and errors, made by classifier. It shows the generalizing property of the separating hyperplane - big values of $C$ corresponds to less generalizing ability and can lead to overfitting, if the data is well described by linear model. To select $C$ one must do cross-validation on validation set to find the best value.\n",
        "\n",
        "### Realisation\n",
        "\n",
        "There are two realisations of linear SVM in sklearn : [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) and [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with *linear* kernel. They build on different libraries, with solve optimisation problem *liblinear* in first case and *libsvm* in second.\n",
        "\n",
        "Here we will use [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with *kernel='linear'*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x1QEJ3FC-f"
      },
      "source": [
        "We generate data samples with:\n",
        "- linearly separable classes\n",
        "- with well separable classes, but not linearly\n",
        "- with non separable classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6NtqMXtFC-f"
      },
      "source": [
        "### The function below provides imaging and data samples creatinon. Your task is to use it according to the next exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJhx_7WsFC-g"
      },
      "outputs": [],
      "source": [
        "# From sklearn SVC examples\n",
        "def svc_plotter(classifiers, name, poly_f=False, polynomial_expand=None):\n",
        "    h = .02  # step size in the mesh\n",
        "        \n",
        "    X_sep, y_sep = make_blobs(n_samples=400, n_features=2, centers=2, cluster_std=1.5, random_state=42)\n",
        "    linearly_separable = (X_sep, y_sep)\n",
        "\n",
        "    X, y = make_classification(n_samples=400, n_features=2, n_redundant=0, n_informative=2,\n",
        "                               random_state=231, n_clusters_per_class=1)\n",
        "\n",
        "    rng = np.random.RandomState(2)\n",
        "    X += 15 * rng.uniform(size=X.shape)\n",
        "    linearly_bad_separable = (X, y)\n",
        "\n",
        "\n",
        "    datasets = [linearly_separable,\n",
        "                linearly_bad_separable,\n",
        "                make_circles(n_samples=400, noise=0.05, random_state=42)]\n",
        "\n",
        "    # iterate over datasets\n",
        "    f, axes = plt.subplots(len(datasets), len(classifiers), figsize=(20, 20))\n",
        "    for ds_cnt, ds in enumerate(datasets):\n",
        "        # preprocess dataset, split into training and test part\n",
        "        X, y = ds\n",
        "        X = StandardScaler().fit_transform(X)\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "            train_test_split(X, y, test_size=.4, random_state=42)\n",
        "\n",
        "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                             np.arange(y_min, y_max, h))\n",
        "        # iterate over classifiers\n",
        "        index = 0\n",
        "        for clf in classifiers:\n",
        "            if poly_f:\n",
        "                clf.fit(polynomial_expand.fit_transform(X_train), y_train)\n",
        "            else:\n",
        "                clf.fit(X_train, y_train)\n",
        "            ax = axes[ds_cnt, index]\n",
        "            index += 1\n",
        "\n",
        "            # Plot the decision boundary. For that, we will assign a color to each\n",
        "            # point in the mesh [x_min, x_max]x[y_amin, y_max].\n",
        "            if hasattr(clf, \"decision_function\"):\n",
        "                if poly_f:\n",
        "                    Z = clf.decision_function(polynomial_expand.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "                else:\n",
        "                    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "            else:\n",
        "                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "            # Put the result into a color plot\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            #ax.pcolormesh(xx, yy, Z > 0, cmap=plt.cm.Paired)\n",
        "            ax.contourf(xx, yy, Z, cmap=cm.RdBu, alpha=.8, linestyles=['--', '-', '--'],levels=[-1, 0, 1])\n",
        "            ax.contour(xx, yy, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
        "\n",
        "            #support vectors\n",
        "#             ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
        "#                     facecolors='none', zorder=10)\n",
        "\n",
        "            if poly_f:\n",
        "                poly_features= polynomial_expand.fit_transform(X_train)\n",
        "                train_classes = [poly_features[~y_train.astype(bool)], poly_features[y_train.astype(bool)]] \n",
        "            else:\n",
        "                train_classes = [X_train[~y_train.astype(bool)], X_train[y_train.astype(bool)]] \n",
        "            if not poly_f:\n",
        "                ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=180,\n",
        "                         facecolors='none', zorder=10, edgecolors='black', linewidths=0.5)\n",
        "            # Plot also the training points\n",
        "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.3, s=36)\n",
        "            # and testing points\n",
        "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
        "                       alpha=0.5, s=36)\n",
        "\n",
        "            ax.set_xlim(xx.min(), xx.max())\n",
        "            ax.set_ylim(yy.min(), yy.max())\n",
        "            ax.set_xlabel(\"x_1\")\n",
        "            ax.set_ylabel(\"x_2\")\n",
        "            ax.set_title(name[index - 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZD9jcJPFC-h"
      },
      "source": [
        "### Task 1\n",
        "\n",
        "Vary parameter C(in log scale [hint.nplogspace) and provide a list of SVMs with linear kernel to function. name is list of strings, denoting different labels(values of c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AuCRF9AFC-i"
      },
      "outputs": [],
      "source": [
        "classifiers = []\n",
        "name = []\n",
        "# Hint: you should add new SVC classifiers to list and come up with the name for each of them. Add the name to the other list)\n",
        "for C in <Hint>: #your code\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "svc_plotter(classifiers, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYSQOFK6FC-j"
      },
      "source": [
        "### Task 2\n",
        "\n",
        "How do the number of support vectors depends on C on different datasets?\n",
        "Extract info about support vectors from SVC and plot a graph: Number of Support Vectors VS value of C for all 3 datasets declared below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbt0RSBdFC-j"
      },
      "outputs": [],
      "source": [
        "X_sep, y_sep = make_blobs(n_samples=400, n_features=2, centers=2, cluster_std=1.5, random_state=42)\n",
        "linearly_separable = (X_sep, y_sep)\n",
        "\n",
        "X, y = make_classification(n_samples=400, n_features=2, n_redundant=0, n_informative=2,\n",
        "                           random_state=231, n_clusters_per_class=1)\n",
        "\n",
        "rng = np.random.RandomState(2)\n",
        "X += 15 * rng.uniform(size=X.shape)\n",
        "linearly_bad_separable = (X, y)\n",
        "\n",
        "\n",
        "datasets = [linearly_separable,\n",
        "            linearly_bad_separable,\n",
        "            make_circles(n_samples=400, noise=0.05, random_state=42)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V79omkKDFC-k"
      },
      "outputs": [],
      "source": [
        "vectors = [[],[],[]]\n",
        "paramd_grid = ?# <your code>\n",
        "for index, dataset in enumerate(datasets):\n",
        "    # YOUR CODE HERE\n",
        "    # Create a list of classifiers\n",
        "    for svc in <your list>:\n",
        "        svc.fit(dataset[0], dataset[1])\n",
        "        vectors[index].append(WHAT?)# <your code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCJEDbLHFC-k"
      },
      "outputs": [],
      "source": [
        "plt.figure\n",
        "plt.plot(paramd_grid, vectors[0], label = 'Dataset_1')\n",
        "plt.plot(paramd_grid, vectors[1], label = 'Dataset_2')\n",
        "plt.plot(paramd_grid, vectors[2], label = 'Dataset_3')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('C(log scale)')\n",
        "plt.ylabel('Number of support vectors')\n",
        "plt.ylim(0, 410)\n",
        "plt.title(\"Dependance of number of support vectors from C\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKTdaQ3FFC-l"
      },
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q42vyn9aFC-l"
      },
      "source": [
        "As you can see, for dataset 3, there is no linear hyperplane(e.g. classes can't be separated with a hyperplane)\n",
        ", thus, to solve this problem, one can change feature space, in which the linear model describe dataset better.\n",
        "\n",
        "$$\\boldsymbol x \\in \\mathbb{R}^d \\mapsto \\phi(\\boldsymbol x) \\in \\mathbb{R}^t$$\n",
        "\n",
        "For example, adding all pairwise product of features: $\\phi(x_1, \\dots, x_d) = (x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2)$ transform to the space, where linear hyperplane is a quadratic form in initial feature space.\n",
        "\n",
        "[Video with demonstration](https://youtu.be/9NrALgHFwTo)\n",
        "<img src=\"https://www.aionlinecourse.com/uploads/tutorials/2019/07/11_Kernel_SVM_5.jpg\" width=50%> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al56okLTFC-m"
      },
      "source": [
        "There is a module in sklearn which will can it for you, and not only. Have a look at it.\n",
        "\n",
        "Use $\\textbf{sklearn.preprocessing PolynomialFeatures(with include_bias=False)}$ and plot the results as above. Briefly describe the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGKJtdnIFC-m",
        "outputId": "b7d78dfa-283a-48db-c12d-7ebff971b0ec"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-33-fd92367e7dca>, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-fd92367e7dca>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    for C in ?:\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "classifiers = []\n",
        "name = []\n",
        "# YOUR CODE HERE\n",
        "for C in ?:\n",
        "  # YOUR CODE HERE (Hint: it is very similar to the frist task)\n",
        "svc_plotter(classifiers, name, poly_f=True, polynomial_expand=<YOUR instance of PolynomialFeatures>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV8w8ovcfckm"
      },
      "source": [
        "We transform out feature space to quadratic feature space where our classes are linearly separable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIe3UUYQFC-n"
      },
      "source": [
        "## Kernel SVM\n",
        "\n",
        "![](http://i.imgur.com/bJAzRCt.png)\n",
        "\n",
        "Linaer SVM problem, covered above, is usually called direct optimisation problem. Any direct problem has $\\textbf{dual}$ problem and in some cases optimums of both problems coincide.\n",
        "\n",
        "\n",
        "Dual problem for SVM is:\n",
        "$$\n",
        "\\begin{gather}\n",
        "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
        "    \\begin{aligned}\n",
        "        \\text{s.t. } \\quad  \n",
        "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
        "        & \\sum_{n} \\alpha_n y_n = 0\n",
        "    \\end{aligned}\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Vector of dual variables is being optimised $\\alpha_n$. Object $x_n$ is a SV, if $\\alpha_n > 0$.\n",
        "\n",
        "The predicted label is given by:\n",
        "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
        "\n",
        "#### Kernel trick\n",
        "Notice, that dual problem has features only as a scalar product $x^Tx'$. This observation helps us to perform kernel trick - implicitly change feature space. Instead of calculating $\\phi(\\boldsymbol x)$ (as we did before) we will compute scalar product $k(\\boldsymbol x, \\boldsymbol x')$ called $\\textbf{kernel}$ and plug it insted of $x^Tx'$ above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpICDdtlFC-n"
      },
      "source": [
        "### Task 4\n",
        "\n",
        "Try different SVM kernel and plot pictures as above, look what other parameters kernels have and how they affect results.\n",
        "- polynomial: $k(x, x') = (x^Tx' + 1)^d$ with different $d = 2,3,\\dots$\n",
        "- Gaussian RBF: $k(x, x') = \\exp(-\\sigma\\|x - x'\\|^2)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TIn1A7IFC-o"
      },
      "source": [
        "## Poly kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoWX7RCcFC-o"
      },
      "source": [
        "### degree=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5M91DPgFC-o"
      },
      "outputs": [],
      "source": [
        "classifiers = []\n",
        "name = []\n",
        "# YOUR CODE HERE\n",
        "svc_plotter(classifiers, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7wECpH0FC-o"
      },
      "source": [
        "### degree=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2th2djEpFC-p"
      },
      "outputs": [],
      "source": [
        "classifiers = []\n",
        "name = []\n",
        "# YOUR CODE HERE\n",
        "svc_plotter(classifiers, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyVFuAV-FC-p"
      },
      "source": [
        "### degree=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HKVRn_HFC-p"
      },
      "outputs": [],
      "source": [
        "classifiers = []\n",
        "name = []\n",
        "# YOUR CODE HERE\n",
        "svc_plotter(classifiers, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnd34ZC1FC-p"
      },
      "source": [
        "## RBF kernel(vary gamma parameter in logscale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elW0SPkFC-q"
      },
      "outputs": [],
      "source": [
        "classifiers = []\n",
        "name = []\n",
        "# YOUR CODE HERE\n",
        "svc_plotter(classifiers, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZg72u_uFC-q"
      },
      "source": [
        "Video with RBF kernel simply explaned https://youtu.be/Qc5IyLW_hns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uRjoD_8fcks"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SVM_seminar.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}