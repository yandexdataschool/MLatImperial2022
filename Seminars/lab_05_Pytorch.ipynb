{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/master/Seminars/lab_05_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbIJ2AEnj33_"
      },
      "source": [
        "# Hello, pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm9CD832ky4o"
      },
      "source": [
        "![img](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)\n",
        "\n",
        "\n",
        "> **!!!** If you're running this notebook on Colab, make sure you enable the GPU support by going to `'Edit'->'Notebook settings'` and setting `'Hardware accelerator'` to `'GPU'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fea1jESIjHPK"
      },
      "source": [
        "[PyTorch](http://pytorch.org/) is one of the most commonly used deep learning frameworks. Pytorch tools are notable for their ability to compute gradients automatically and do operations on GPU, which can be by orders of magnitude faster than running on CPU.\n",
        "\n",
        "In pytorch you can compute outputs on the fly without pre-declaring anything, and the code looks (almost) exactly as in pure numpy. Due to this simplicity, PyTorch is going to be our weapon of choice for this course.\n",
        "\n",
        "We'll start from using the low-level core of PyTorch, and then try out some high-level features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNqZu7dvjHPO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxIJVjXajHPS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# numpy operations\n",
        "x = np.arange(16).reshape(4, 4)\n",
        "\n",
        "print(f\"X :\\n{x}\\n\")\n",
        "print(f\"X.shape : {x.shape}\\n\")\n",
        "print(f\"X.shape + dummy dim: {x[None, :, :].shape}\\n\") # or np.newaxis\n",
        "print(f\"add 5 :\\n{x + 5}\\n\")\n",
        "print(f\"X*X^T  :\\n{np.dot(x, x.T)}\\n\")\n",
        "print(f\"mean along cols :\\n{x.mean(axis=1)}\\n\")\n",
        "print(f\"cumsum of cols :\\n{np.cumsum(x, axis=1)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4CU3mBTjHPW"
      },
      "outputs": [],
      "source": [
        "# pytorch operations\n",
        "x = torch.arange(16.0).reshape(4, 4) # What if we pass 16 instead of 16.0?\n",
        "\n",
        "print(f\"X :\\n{x}\\n\")\n",
        "print(f\"X.shape : {x.shape}\\n\")\n",
        "print(f\"X.shape + dummy dim: {x.unsqueeze(0).shape}\\n\")\n",
        "print(f\"add 5 :\\n{x + 5}\")\n",
        "print(f\"X*X^T  :\\n{torch.matmul(x, x.T)}\\n\") # torch.dot assumes arguments are 1D vectors (does inner product)\n",
        "print(f\"mean along cols :\\n{x.mean(dim=1)}\\n\")\n",
        "print(f\"cumsum of cols :\\n{x.cumsum(dim=1)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m99e3Jor8JNl"
      },
      "source": [
        "### Converting numpy <--> pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBb-sAeq8JNl"
      },
      "outputs": [],
      "source": [
        "X_numpy = np.array([1, 2, 3])\n",
        "X_torch = torch.from_numpy(X_numpy)\n",
        "X_numpy_again = X_torch.numpy()\n",
        "\n",
        "X_numpy, X_torch, X_numpy_again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiNuHgk68JNm"
      },
      "source": [
        "### Sending tensors to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww6jO8Z98JNm"
      },
      "outputs": [],
      "source": [
        "print(f\"cuda is available: {torch.cuda.is_available()}\")\n",
        "X_gpu = X_torch.cuda() # .to(device), device = 'cpu' or 'cuda'\n",
        "print(X_gpu)\n",
        "\n",
        "# Let's convert to numpy\n",
        "X_gpu.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_gpu.cpu().numpy(), X_gpu.device, X_torch.device)"
      ],
      "metadata": {
        "id": "MiRtChmi8jSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYWpOODA8JNn"
      },
      "source": [
        "### Types in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0iZkyEs8JNn"
      },
      "outputs": [],
      "source": [
        "values = [-1, 2, -3, 4]\n",
        "\n",
        "X_float = torch.tensor(values, dtype=torch.float)\n",
        "X_float2 = torch.tensor(values).float()\n",
        "\n",
        "X_long = torch.tensor(values, dtype=torch.long) # torch.int is int32, torch.long is int64\n",
        "X_long2 = torch.tensor(values).long()\n",
        "\n",
        "\n",
        "print(X_float, X_float2)\n",
        "print(X_long, X_long2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_float.long())"
      ],
      "metadata": {
        "id": "ctRzCuRQ9avc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAOmRYq7ui11"
      },
      "source": [
        "## NumPy and Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V4RORsfjHPY"
      },
      "source": [
        "As you can notice, pytorch allows you to write code much in the same way you did with numpy. But Numpy does not have GPU support, autogradient calculations and many other tools you need for DL. But Pytorch does!\n",
        "\n",
        "You may also see a few new method names in Pytorch, but mainly they are similar to Numpy. Get excited!\n",
        "\n",
        "![img](http://i0.kym-cdn.com/entries/icons/original/000/017/886/download.jpg)\n",
        "\n",
        "\n",
        "To help you acclimatize, there's a [table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) covering most new things. There's also a neat [documentation page](http://pytorch.org/docs/master/).\n",
        "\n",
        "Finally, if you're stuck with a technical problem, we recommend searching [pytorch forums](https://discuss.pytorch.org/). Or just googling, which usually works just as efficiently. \n",
        "\n",
        "If you feel like you almost give up, remember two things: **GPU** and **gradients for free**. Besides you can always jump back to numpy with x.numpy()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RcZjFrcjHPa"
      },
      "source": [
        "### Warmup: analytical solution to Ridge regression\n",
        "$$\n",
        "X \\in \\mathbb{R}^{N \\times D}, \\ y \\in \\mathbb{R}^{N}, \\theta \\in \\mathbb{R}^{D}\\\\\n",
        "\\mathcal{L}_{\\theta} = \\sum_{i=1}^{N} (<X_{i}, \\theta>  - y_{i})^{2} + \\lambda \\sum_{i=1}^{D} \\theta_{i}^{2} \\rightarrow \\min_{\\theta}\n",
        "$$\n",
        "Stationary condition:\n",
        "$$\n",
        "\\nabla_{\\theta}\\mathcal{L}_{\\theta} = 2\\sum_{i=1}^{N} X_{i}(<X_{i}, \\theta>  - y_{i}) + 2\\lambda\\theta = \\overline{0}\n",
        "$$\n",
        "Stationary condition in matrix notation:\n",
        "$$\n",
        "\\nabla_{\\theta}\\mathcal{L}_{\\theta} = 2 X^{T}(X\\theta  - y) + 2\\lambda\\theta = \\overline{0} \\\\\n",
        "$$\n",
        "Solution:\n",
        "$$\n",
        "(X^{T}X + \\lambda\\mathrm{I}) \\theta = X^{T}y\\\\\n",
        "\\theta = (X^{T}X + \\lambda\\mathrm{I})^{-1}X^{T}y\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKHJfVhlhWw_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "N, D = 100, 10\n",
        "\n",
        "X = torch.randn((N, D))\n",
        "y = torch.randn((N, 1))\n",
        "lambda_ = 1.0\n",
        "\n",
        "inv_term = <YOUR_CODE> # use torch.linalg.inv\n",
        "theta = <YOUR_CODE>\n",
        "\n",
        "# Compare our solution with sklearn Ridge solver\n",
        "model = Ridge(lambda_, fit_intercept=False)\n",
        "model.fit(X.numpy(), y.numpy())\n",
        "\n",
        "#Check if solutions are close\n",
        "assert np.allclose(theta.numpy(), model.coef_.T, atol=1e-6), \"Theta is not a solution\"\n",
        "print(\"Yes, they are close!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2uMMOF7jHPj"
      },
      "source": [
        "## Automatic gradients\n",
        "\n",
        "Any self-respecting DL framework must do your backprop for you. Torch handles this through `requires_grad` parameter when creating tensors.\n",
        "\n",
        "The general pipeline looks like this:\n",
        "* You create ```a = torch.tensor(data, requires_grad=True)```\n",
        "* You define some differentiable `loss = whatever(a)`\n",
        "* Call `loss.backward()`\n",
        "* Gradients are now available as ```a.grad```\n",
        "\n",
        "**Here's an example:** let's fit a linear regression on Boston house prices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate gradients"
      ],
      "metadata": {
        "id": "J-4JirUb_92t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz\n",
        "from torchviz import make_dot"
      ],
      "metadata": {
        "id": "e4GFgKC6CGkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2.0], requires_grad=True) # What if dtype is integer\n",
        "b = torch.tensor([10.0], requires_grad=False)\n",
        "\n",
        "c = b * a**2\n",
        "# analytically:\n",
        "#   dc/da = 2 * a * b = 40\n",
        "#   dc/db = a * a = 4\n",
        "print(a.grad, b.grad)\n",
        "c.backward()\n",
        "print(a.grad, b.grad)"
      ],
      "metadata": {
        "id": "nvXiVRB2ACpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "c = mul(b, pow(a, 2))\\\\\n",
        "dc = dmul(b, pow(a, 2)) = (pow(a, 2))db + b * dpow(a, 2) = (pow(a, 2))db + (2 * b * a)da \\\\\n",
        "\\dfrac{dc}{da} = \\{\\dfrac{db}{da} = 0, \\dfrac{da}{da} = 1\\} = 2 * a * b \\\\ \n",
        "\\dfrac{dc}{db} = \\{\\dfrac{da}{db} = 0, \\dfrac{db}{db} = 1\\} = pow(a, 2) = a * a \\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "76b6YiJnD6vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_dot(c)"
      ],
      "metadata": {
        "id": "TIaECvwXBPpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2.0]*10, requires_grad=True)\n",
        "b = torch.tensor([10.0]*10, requires_grad=True)\n",
        "\n",
        "c = b * a**2\n",
        "# analytically:\n",
        "#   dc/da = 2 * a * b = 40\n",
        "#   dc/db = a * a = 4\n",
        "c.backward(torch.ones(c.shape[0])) # this way it will compute element-wise grad([1.0*c, 1.0*c, ..., 1.0*c]), same shape as c\n",
        "print(a.grad, b.grad)"
      ],
      "metadata": {
        "id": "3EWnCkp-GF_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_dot(c)"
      ],
      "metadata": {
        "id": "nSYHwNKgG2zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detaching"
      ],
      "metadata": {
        "id": "gDqYDLx3LWrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(c)\n",
        "print(c.detach().numpy())"
      ],
      "metadata": {
        "id": "hZlbReQOLZoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boston dataset"
      ],
      "metadata": {
        "id": "xnm4WHPQHDda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCp_lBj9jHPk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "# Let's have a look at the last feature:\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(boston.data[:, -1], boston.target);\n",
        "plt.xlabel('Last feature')\n",
        "plt.ylabel('Target')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOFb6pCQl-P2"
      },
      "outputs": [],
      "source": [
        "# Defining the parameters of our model:\n",
        "w = torch.zeros((1, ), requires_grad=True) # float by default\n",
        "b = torch.zeros((1, ), requires_grad=True)\n",
        "\n",
        "# Converting our data to torch tensors:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = torch.from_numpy(StandardScaler().fit_transform(boston.data)[:,-1]) # select last feature\n",
        "y = torch.from_numpy(boston.target)\n",
        "\n",
        "print(f\"X.shape : {X.shape}\\ny.shape: {y.shape}\\nw.shape : {w.shape}\\nb.shape : {b.shape}\\n\")\n",
        "\n",
        "# prediction:\n",
        "y_pred = X*w + b\n",
        "# loss:\n",
        "loss = ((y_pred - y)**2).mean()\n",
        "print(\"Loss is:\", loss)\n",
        "\n",
        "# Compute gradients:\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Fy8EXAjHPu"
      },
      "source": [
        "The gradients are now stored in `.grad` of a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuA8Q3Y3jHPv"
      },
      "outputs": [],
      "source": [
        "print(\"dL/dw =\", w.grad)\n",
        "print(\"dL/db =\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMXEd1etjHPz"
      },
      "source": [
        "If you compute gradients from multiple losses, the gradients will add up at variables, therefore you usually want to **zero the gradients** between iteratons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV14F3VejHP0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# let's keep the history of loss values at \n",
        "# each step\n",
        "losses = []\n",
        "lr = 0.05 # learning rate\n",
        "\n",
        "w_grads = []\n",
        "b_grads = []\n",
        "\n",
        "# Gradient descent:\n",
        "for i in range(50):\n",
        "    # Calculate current prediction and loss:\n",
        "    y_pred = w * X  + b\n",
        "    loss = ((y_pred - y)**2).mean()\n",
        "    losses.append(loss.item())\n",
        "    #                   ^^^\n",
        "    # The '.item' call returns the value of this \n",
        "    # tensor as a standard Python number (this only works\n",
        "    # for tensors with one element).\n",
        "\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    w_grads.append(w.grad.data.item())\n",
        "    b_grads.append(b.grad.data.item())\n",
        "    with torch.no_grad():\n",
        "        # ^^^ this `with` statement ensures that\n",
        "        #     automatic gradients will not be computed\n",
        "        #     for whatever is written in the current block:\n",
        "        w -= lr * w.grad\n",
        "        b -= lr * b.grad\n",
        "\n",
        "\n",
        "    # manually zero the gradients\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # the rest of code is just bells and whistles\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # We'll draw the data points and fit result on the left subplot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"data\")\n",
        "    plt.scatter(X.numpy(), y.numpy())\n",
        "    plt.scatter(X.numpy(), y_pred.detach().numpy(),\n",
        "                color='orange', linewidth=5)\n",
        "    # The '.detach()' call is needed here to create a\n",
        "    # copy of 'y_pred' that does not require gradients\n",
        "    # (otherwize it's impossible to get a numpy\n",
        "    # representation).\n",
        "\n",
        "    # We'll draw the loss (as the function of gradient descent\n",
        "    # step) on the right subplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"loss\")\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('step')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(\"loss = \", loss.detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W grads: \", w_grads[:5])\n",
        "print(\"b grads: \", b_grads[:5])"
      ],
      "metadata": {
        "id": "K85FxAdTMeK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8k2EmXwjHP6"
      },
      "source": [
        "try implementing and writing some nonlinear regression. You can try quadratic features or some trigonometry, or a simple neural network. The only difference is that now you have more variables and a more complicated `y_pred`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K-NeHMF19Dq"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# let's keep the history of loss values at \n",
        "# each step\n",
        "losses = []\n",
        "lr = 0.05\n",
        "\n",
        "powers = range(0, 3)\n",
        "w = torch.zeros(len(powers), requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "# Implement feature transformation: X_p = [X^0, X^1, X^2] <-- column-wise\n",
        "X_p = <YOUR_CODE> # power of 0, 1, 2\n",
        "\n",
        "# Gradient descent:\n",
        "for i in range(100):\n",
        "    # Calculate current prediction and loss:\n",
        "    y_pred = <YOUR_CODE>\n",
        "    loss = ((y_pred - y)**2).mean()\n",
        "    losses.append(loss.item())\n",
        "    #                   ^^^\n",
        "    # The '.item' call returns the value of this \n",
        "    # tensor as a standard Python number (this only works\n",
        "    # for tensors with one element).\n",
        "  \n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "    # ^^^ this `with` statement ensures that\n",
        "    #     automatic gradients will not be computed\n",
        "    #     for whatever is written in the current block:\n",
        "        w -= lr  * w.grad\n",
        "\n",
        "\n",
        "    # manually zero the gradients\n",
        "    w.grad.zero_()\n",
        "\n",
        "    # the rest of code is just bells and whistles\n",
        "    if (i + 1) % 5 == 0:\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(14, 5))\n",
        "\n",
        "        # We'll draw the data points and fit result on the left subplot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"data\")\n",
        "        plt.scatter(X.numpy(), y.numpy())\n",
        "        plt.scatter(X.numpy(), y_pred.detach().numpy(),\n",
        "                    color='orange', linewidth=5)\n",
        "        # The '.detach()' call is needed here to create a\n",
        "        # copy of 'y_pred' that does not require gradients\n",
        "        # (otherwize it's impossible to get a numpy\n",
        "        # representation).\n",
        "\n",
        "        # We'll draw the loss (as the function of gradient descent\n",
        "        # step) on the right subplot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"loss\")\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('step')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(\"loss = \", loss.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjrsEoVl6gm5"
      },
      "source": [
        "## High-level PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOqcKJuLjD5I"
      },
      "source": [
        "We're now going to teach the computer to identify some clothes. Exciting, isn't it? The dataset we'll use for that is called [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist). Here's an excerpt from their website:\n",
        "\n",
        "> *Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.*\n",
        "\n",
        "So, if you're familiar with the original [MNIST](http://yann.lecun.com/exdb/mnist/) dataset (handwritten digits), this is kinda the same, but about clothes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj2cI3PtXPCh"
      },
      "outputs": [],
      "source": [
        "#!rm -r FashionMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegU7EdXkzD4"
      },
      "source": [
        "PyTorch has a module dedicated to retrieving datasets, so let's use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6usBsSGi6k6u"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r49OsLdH6yRo"
      },
      "outputs": [],
      "source": [
        "# Getting the train and test parts of the dataset\n",
        "data_train = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=True)\n",
        "\n",
        "data_test = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=False)\n",
        "\n",
        "# In fact, it's already stored as torch tensor, but we'll need\n",
        "# to work with the numpy representation, so let's do the convertion:\n",
        "X_train = data_train.train_data.numpy()\n",
        "y_train = data_train.train_labels.numpy()\n",
        "\n",
        "X_test = data_test.test_data.numpy()\n",
        "y_test = data_test.test_labels.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-VvVjxj8IEk"
      },
      "source": [
        "The datasets consists of images belonging to one out of 10 classes:\n",
        "\n",
        "| Label | Description | | Label | Description |\n",
        "| ---       \n",
        "| 0        | T-shirt/top   || 5        | Sandal         |\n",
        "| 1        | Trouser        || 6        | Shirt             |\n",
        "| 2        | Pullover       || 7        | Sneaker       |\n",
        "| 3        | Dress           || 8        | Bag              |\n",
        "| 4        | Coat             || 9        | Ankle boot  |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNk7BuyQnHyF"
      },
      "source": [
        "Let's check out the shapes of our arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWq9TWKVCPeE"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb9rebjbnNLd"
      },
      "source": [
        "And now we are going to plot images from specific categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHvXHn7D7PmY"
      },
      "outputs": [],
      "source": [
        "categories = [\n",
        "    X_train[y_train == i]\n",
        "    for i in range(10)\n",
        "]\n",
        "\n",
        "ten_of_each = np.array([c[:10] for c in categories])\n",
        "ten_of_each = np.transpose(ten_of_each, (0, 2, 1, 3)).reshape(280, 280)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(ten_of_each, cmap='Greys')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0vyO5SSpCF7"
      },
      "source": [
        "Ok, so let's start writing our first deep learning model! For that we'll need 3 things:\n",
        "  - a **loss** function that takes predicted class scores and true labels, and outputs a measure of how wrong we are in our predictions\n",
        "  - a **function** that converts the input features to class scores (this will be represented by a neural network)\n",
        "  - an **optimization algorithm** that'll vary the parameters of our model to minimize the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOWxl3nAALEY"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWvwI3snrz2z"
      },
      "source": [
        "Since we are facing a classification problem, **cross-entropy** (aka negative log likelihood) is a good choice for a loss function:\n",
        "$$\\mathscr{L} = -\\frac{1}{|D|}\\sum_{c\\in C,\\,i\\in D}p^{\\text{true}}_{c,i}\\cdot log\\,\\hat{p}^{\\text{predicted}}_{c,i},$$\n",
        "where $C$ is the set of all classes and $D$ is the set of all objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqVsCmNDCyD6"
      },
      "source": [
        "<font color='red'>Question:</font> *What is $p^{\\text{true}}_{c,i}$ equal to for a given object $i$ of class $c'$?*\n",
        "<font color='red'>Question (2):</font> *What will $\\mathscr{L}$ look like for a binary classification case?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cJm2yA4YDU1"
      },
      "source": [
        "Since our loss function takes in *probabilities*, we have to make sure they add up to 1 for any given object. In general case, the output of a neural net can be anything, but there's a commonly used activation function that automatically normalizes the outputs to add up to 1:\n",
        "\n",
        "$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
        "\n",
        "This is the *softmax function*, and we'll use it as the activation for the very last layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2beWS7gbIND"
      },
      "source": [
        "Let's start by trying to implement this function (in torch):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9C-cwiTniWQ"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    <YOUR_CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO9p5Mr-Si_X"
      },
      "source": [
        "Once you're done, run the cell below to check whether your function passes automatic checks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQGL9r8Kbwny"
      },
      "outputs": [],
      "source": [
        "# Creating some simple input\n",
        "dummy_input_1 = torch.tensor([0.5, -10., 4., 1.333])\n",
        "dummy_input_2 = 100 * dummy_input_1\n",
        "\n",
        "# Computing the output with your implementation\n",
        "dummy_result_1 = softmax(dummy_input_1)\n",
        "dummy_result_2 = softmax(dummy_input_2)\n",
        "\n",
        "# Checking the output type\n",
        "assert isinstance(dummy_result_1, torch.Tensor), \\\n",
        "       \"Your function should return a torch tensor\"\n",
        "\n",
        "# Correct outputs (for the automatic checks):\n",
        "dummy_correct_result_1 = torch.tensor([2.7460692e-02,\n",
        "                                   7.5616998e-07,\n",
        "                                   9.0937322e-01,\n",
        "                                   6.3165329e-02])\n",
        "dummy_correct_result_2 = torch.tensor([0., 0., 1., 0.])\n",
        "\n",
        "# Doing the checks:\n",
        "assert torch.allclose(\n",
        "            dummy_result_1,\n",
        "            dummy_correct_result_1\n",
        "       ), \"Test failed!\\n\" \\\n",
        "       \"Looks like something is wrong \" \\\n",
        "       \"with your softmax implementation.\\nCorrect result is:\\n{}\\n\" \\\n",
        "       \"You got:\\n{}\".format(dummy_correct_result_1, dummy_result_1)\n",
        "\n",
        "assert torch.isfinite(dummy_result_2).all(), \\\n",
        "       \"Test failed!\\n\" \\\n",
        "       \"Looks like your implementation is unstable to large inputs.\\n\" \\\n",
        "       \"You got:\\n{}\\n\" \\\n",
        "       \"Can you think of a way of fixing it?\".format(dummy_result_2)\n",
        "\n",
        "assert torch.allclose(\n",
        "            dummy_result_2,\n",
        "            dummy_correct_result_2\n",
        "       ), \"Test failed!\\n\" \\\n",
        "       \"Looks like something is wrong \" \\\n",
        "       \"with your softmax implementation.\\nCorrect result is:\\n{}\\n\" \\\n",
        "       \"You got:\\n{}\".format(dummy_correct_result_2, dummy_result_2)\n",
        "\n",
        "# Printout for the case when all is good.\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F187gI18UOq3"
      },
      "source": [
        "Were you able to regularize your softmax implementation?\n",
        "\n",
        "In fact, since we are combining crossentropy and softmax, we'll always take a `log` of the softmax output (see formulas above). This means one more level of numerical regularisation of the final expression can be done. This has already been implemented in the high-level PyTorch `nn` module within the `torch.nn.CrossEntropyLoss` class.\n",
        "\n",
        "Let's use it to create our loss function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlX5RdtoXDps"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function:\n",
        "loss_function = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fem8Z1kZLbA"
      },
      "source": [
        "The `loss_function` now takes two arguments as its input: score predictions (output of the last layer, without activation) and the correct labels:\n",
        "\n",
        "$$\\mathscr{L}(x, c) = -\\frac{1}{|D|}\\sum_{i\\in D}log\\,\\frac{e^{x_i^{c_i}}}{\\sum_{j\\in C}e^{x_i^j}},$$\n",
        "\n",
        "where $x_i^j$ is the score for the $i$-th object to belong the $j$-th class, $c_i$ is the true class for $i$-th object; $C$ is the set of all classes and $D$ is the set of all objects\n",
        "\n",
        "Below is an example of how `loss_function` can now be used.\n",
        "\n",
        "**Task:** try changing the values of `dummy_x` to make the resulting loss even smaller (min loss value is 0.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Cps7YwVnTc"
      },
      "outputs": [],
      "source": [
        "# Example usage (play around with this code)\n",
        "\n",
        "# Think of dummy_x as of output of the last layer of our neural net.\n",
        "# For example, dummy_x below corresponds to a 3-class classification of\n",
        "# 5 objects:\n",
        "dummy_x = torch.tensor(\n",
        "    [[9., -5., -10.], # here we're almost sure it's class 0\n",
        "     [-3.,  8.,   9.], # here we think it's either class 1 or 2, etc\n",
        "     [-3., 15., -10.],\n",
        "     [ 4.,  5.,  -3.],\n",
        "     [-3., 15., -10.]]\n",
        ")\n",
        "# And these are the answers (vector of class ids):\n",
        "dummy_y = torch.tensor([0, 2, 1, 1, 1])\n",
        "\n",
        "print(loss_function(dummy_x, dummy_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNf0CX-fAZfH"
      },
      "source": [
        "### Neural net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WFbVrgutsuf"
      },
      "source": [
        "Now that we have a loss function, it's time to build our network. Again, there's a bunch of ready to use classes in the `torch.nn` module.\n",
        "\n",
        "Let's start from something really simple - a model without any hidden layers. For such a case we should only have 1 input layer, 1 output layer and a set of linear connections between them:\n",
        "\n",
        "$$\\text{output} = XW_{linear} + b_{linear}$$\\\\\n",
        "\n",
        "$$\n",
        "X \\in \\mathbb{R}^{N \\ \\times \\ \\text{input_dim}}\\\\\n",
        "W_{linear}\\in \\mathbb{R}^{\\text{input_dim}\\  \\times \\ \\text{output_dim}}\\\\\n",
        "b_{linear} \\in \\mathbb{R}^{\\text{output_dim}}\\\\\n",
        "$$\n",
        "\n",
        "We'll use the `torch.nn.Linear` class to define these connections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgvwEXm4vWSX"
      },
      "outputs": [],
      "source": [
        "input_dim = 28 * 28 # number of pixels per image\n",
        "output_dim = 10 # number of classes\n",
        "model = torch.nn.Linear(in_features=input_dim,\n",
        "                        out_features=output_dim).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwGSePvGyxkj"
      },
      "source": [
        "Here's an example of how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb8tip2XyUOz"
      },
      "outputs": [],
      "source": [
        "# You can think of dummy_input as of 5 images with all pixels black:\n",
        "dummy_input = torch.zeros((5, 28 * 28)).cuda()\n",
        "# Then our output is going to be the 10 class scores for each of the images:\n",
        "dummy_output = model(dummy_input)\n",
        "\n",
        "# Let's have a look at their shape:\n",
        "print(dummy_input.shape)\n",
        "print(dummy_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvJaQTUf8JN6"
      },
      "source": [
        "**Question:** how many parameters does this model have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJucbQGHzoQH"
      },
      "source": [
        "One can iterate through the `model`'s parameters with it's `.parameters()` method. Try using it to calculate the number of (scalar) parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMRzL12s0iHn"
      },
      "outputs": [],
      "source": [
        "# calculate the number of (scalar) parameters:\n",
        "n_parameters = 0\n",
        "for parameter in model.parameters():\n",
        "    n_parameters += parameter.reshape(-1).shape[0]\n",
        "\n",
        "print(n_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSPYLKzJAgzO"
      },
      "source": [
        "### Building dataset and Input preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl-0UxIJxTkn"
      },
      "source": [
        "So far our data is held as numpy arrays of unsigned byte type, i.e. it lies within a range from 0 to 255. Also, the shape of our input is 3-dimensional (num_images, height, width), while our `model` takes 2-dimensional \"arrays of 1-dimensional images\" (num_images, height * width)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-e6X4v6wlJM"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_train.dtype)\n",
        "print(X_train.min(), X_train.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF2nptPHx3-g"
      },
      "source": [
        "We have to create special dataset class that converts data to `torch` tensors, normalizes it to $[0, 1]$ interval and reshapes the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhWcXNTy260C"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class FashionMNISTDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X, self.y = self.preprocess_data(X, y)\n",
        "        \n",
        "    def preprocess_data(self, X, y):\n",
        "        X_preprocessed = torch.tensor(X / 255.,\n",
        "                                    dtype=torch.float).reshape(-1, 28 * 28).cuda()\n",
        "        \n",
        "        if (y is None):\n",
        "            return X_preprocessed, None\n",
        "        \n",
        "        return X_preprocessed, torch.tensor(y).cuda()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if (self.y is None):\n",
        "            return self.X[idx]\n",
        "        \n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3-ayFGI8JN7"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "ds_train = FashionMNISTDataset(X_train, y_train)\n",
        "ds_test = FashionMNISTDataset(X_test, y_test)\n",
        "\n",
        "dl_train = DataLoader(ds_train, batch_size = BATCH_SIZE, shuffle=True)\n",
        "dl_test = DataLoader(ds_test, batch_size = BATCH_SIZE, shuffle=False)\n",
        "y_test = torch.tensor(y_test).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIUhQLYN_4mL"
      },
      "source": [
        "The simplest way to check this function is to use it to calculate loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgR5FZus3CRW"
      },
      "outputs": [],
      "source": [
        "sample_X, sample_y = next(iter(dl_train))\n",
        "\n",
        "assert sample_X.min().item() >= 0 and \\\n",
        "       sample_X.max().item() <= 1, \"Make sure your input is >= 0 and <= 1\"\n",
        "\n",
        "class_scores = model(sample_X)\n",
        "print(\"Loss is:\", loss_function(class_scores, sample_y).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nybq16Ak7a"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4HrUU_rA_M1"
      },
      "source": [
        "Now, to train our network we need an optimization algorithm. \n",
        "\n",
        "Actually, we already implemented one ourselves: stochastic gradient descent (from the *Automatic gradients* section of this notebook). Back then we manually updated our model's parameters by adding a `-learning_rate * param.grad` at each step.\n",
        "\n",
        "There are however other extensions of stochastic gradient descent that compute the per-step updates using more complicated functions of gradients (some take into account the gradients' values at earlier steps).\n",
        "\n",
        "As always, many of those are already implemented in torch. In this example we are going to use `torch.optim.Adam`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtlkvLG6F3XK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0005\n",
        "# When creating an istance of the optimizer you have to tell\n",
        "# it which parameters you want to optimize. We are doing so by\n",
        "# passing model.parameters() to it:\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrBk5DCOGul-"
      },
      "source": [
        "Once an optimizer is created we can use it by first calculating the loss on some batch of data, computing the gradients and then simply calling to `optimizer.step()`. Then repeat with the next batch and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ3LwF9h8JN8"
      },
      "source": [
        "### Build pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J4oPwJYkSYG"
      },
      "source": [
        "Ideally we would like to loop through all of our train data. One such loop is called an 'epoch'. We'd also like the data to be shuffled at each epoch.\n",
        "\n",
        "Let's write a function that gives us batches of data and which we'll call at the start of each epoch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDLRDuFVrzX9"
      },
      "source": [
        "We'll add one more utility to keep track of loss values and plot them at each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nxVgdqy8JN8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self.train_loss_batch = []\n",
        "        self.train_loss_epoch = []\n",
        "        self.test_loss_batch = []\n",
        "        self.test_loss_epoch = []\n",
        "        self.train_batches_per_epoch = 0\n",
        "        self.test_batches_per_epoch = 0\n",
        "        self.epoch_counter = 0\n",
        "\n",
        "    def fill_train(self, loss):\n",
        "        self.train_loss_batch.append(loss)\n",
        "        self.train_batches_per_epoch += 1\n",
        "\n",
        "    def fill_test(self, loss):\n",
        "        self.test_loss_batch.append(loss)\n",
        "        self.test_batches_per_epoch += 1\n",
        "\n",
        "    def finish_epoch(self):\n",
        "        self.train_loss_epoch.append(np.mean(\n",
        "            self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "        ))\n",
        "        self.test_loss_epoch.append(np.mean(\n",
        "            self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "        ))\n",
        "        self.train_batches_per_epoch = 0\n",
        "        self.test_batches_per_epoch = 0\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8}\".format(\n",
        "                  self.epoch_counter,\n",
        "                  self.train_loss_epoch[-1],\n",
        "                  self.test_loss_epoch [-1]\n",
        "              ))\n",
        "\n",
        "        self.epoch_counter += 1\n",
        "\n",
        "        plt.figure(figsize=(11, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_loss_batch, label='train loss')\n",
        "        plt.xlabel('# batch iteration')\n",
        "        plt.ylabel('loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "        plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "        plt.legend()\n",
        "        plt.xlabel('# epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WZ_rPJe8JN8"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.m =  torch.nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.m(X)\n",
        "    \n",
        "def train(model, optimizer, dl_train, dl_test, criterion, n_epochs):\n",
        "    logger = Logger()\n",
        "    \n",
        "    for i_epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in dl_train:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = criterion(model(batch_X), batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            logger.fill_train(loss.item())\n",
        "            \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y  in dl_test:\n",
        "                loss = criterion(model(batch_X), batch_y)\n",
        "                logger.fill_test(loss.item())\n",
        "\n",
        "        logger.finish_epoch()\n",
        "        \n",
        "def predict(model, dl_test):\n",
        "    model.eval()\n",
        "    prediction = torch.zeros((len(dl_test.dataset), ), dtype=torch.long).cuda()\n",
        "    idx = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X , _ in dl_test:\n",
        "            pred = model(batch_X).squeeze()\n",
        "            size = pred.shape[0]\n",
        "            prediction[idx:idx + size] = torch.argmax(pred, dim=1)\n",
        "            idx += size\n",
        "    \n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_2R-RM6Am3C"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNSaIcwmEKU"
      },
      "source": [
        "Here's the setting up part, copy-pasted from previous cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv6R9kqbl5Of"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_dim = 28 * 28 # number of pixels per image\n",
        "output_dim = 10 # number of classes\n",
        "model = Model(input_dim, output_dim).cuda()\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, dl_train, dl_test, criterion, n_epochs = 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XvdiR4-m8DH"
      },
      "source": [
        "Now it's your turn to code the learning process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2kPqiJ38JN9"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(y_pred, y_test):\n",
        "    return (y_pred == y_test).sum()/len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCv7jQTlw8oC"
      },
      "outputs": [],
      "source": [
        "accuracy_score(predict(model, dl_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ63Xs9q8JN9"
      },
      "source": [
        "### Bonus part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZoCgsSoPWVZ"
      },
      "source": [
        "Ok, so what we did so far is just a simple linear model. Let's add a hidden layer to turn it into a 'real' neural net. Modify the code above.\n",
        "\n",
        "You can stack layers by using `torch.nn.Sequential` class:\n",
        "\n",
        "```\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(...),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(...),\n",
        ")\n",
        "```\n",
        "\n",
        "In this example we've put an activation layer `torch.nn.ReLU` between the two linear ones. What will happen if we don't do that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4gACX3v8JN9"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.m = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, input_dim//4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(input_dim//4, input_dim//8),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(input_dim//8, output_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.m(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoFzT5Vb8JN-"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_dim = 28 * 28 # number of pixels per image\n",
        "output_dim = 10 # number of classes\n",
        "\n",
        "model = Model(input_dim, output_dim).cuda()\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, dl_train, dl_test, criterion, n_epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClKuHnUHRzcU"
      },
      "outputs": [],
      "source": [
        "accuracy_score(predict(model, dl_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tomorrow: Convolution operation\n",
        "In case you're already intrested:\n",
        "- [Pytorch Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "- [About convolution](https://cs231n.github.io/convolutional-networks/)"
      ],
      "metadata": {
        "id": "x3TUqDM3QfBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m8QCh5d0Qiy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab_05_Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}